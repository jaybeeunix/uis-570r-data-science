{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6, Logistic Regression with the Titanic\n",
    "UIS CSC 570R - Data Science Essentials<br>\n",
    "2017 Fall<br>\n",
    "Jason Burrell<br>\n",
    "\n",
    "Logistic regression with the Titanic survivor's data, based on https://github.com/mbernico/CS570/blob/master/module_2/Week6Lecture.ipynb\n",
    "\n",
    "## Predicting Survival on the Titanic using Logistic Regression\n",
    "\n",
    "This week we will be building a logistic regression classifier to predict survival on the titanic.   \n",
    "\n",
    "My model will use the independent variables sex and age to predict the dependent variable survived.  There are many other variables in the dataset that you could and should use, that will be your assignment for the week.   (More on that later)\n",
    "\n",
    "###Data Prep\n",
    "First, I will start with some data prep to get my data ready to be used in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data from the disk into memory\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just a reminder, here are all the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm going to create a new dataframe and put only the three variables I'm going to be using into it.\n",
    "X = pd.DataFrame()\n",
    "X['sex'] = df['Sex']\n",
    "X['age'] = df['Age']\n",
    "X['survived'] = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm going to drop missing values.   That's probably NOT the best strategy, but it's usually good to start simple and \n",
    "#build complexity as you go.\n",
    "X = X.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#survived will be my dependent variable, y.   I'll assign it to y and remove it from X\n",
    "y = X['survived']\n",
    "X = X.drop(['survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     female  male\n",
       "0         0     1\n",
       "1         1     0\n",
       "2         1     0\n",
       "3         1     0\n",
       "4         0     1\n",
       "6         0     1\n",
       "7         0     1\n",
       "8         1     0\n",
       "9         1     0\n",
       "10        1     0\n",
       "11        1     0\n",
       "12        0     1\n",
       "13        0     1\n",
       "14        1     0\n",
       "15        1     0\n",
       "16        0     1\n",
       "18        1     0\n",
       "20        0     1\n",
       "21        0     1\n",
       "22        1     0\n",
       "23        0     1\n",
       "24        1     0\n",
       "25        1     0\n",
       "27        0     1\n",
       "30        0     1\n",
       "33        0     1\n",
       "34        0     1\n",
       "35        0     1\n",
       "37        0     1\n",
       "38        1     0\n",
       "..      ...   ...\n",
       "856       1     0\n",
       "857       0     1\n",
       "858       1     0\n",
       "860       0     1\n",
       "861       0     1\n",
       "862       1     0\n",
       "864       0     1\n",
       "865       1     0\n",
       "866       1     0\n",
       "867       0     1\n",
       "869       0     1\n",
       "870       0     1\n",
       "871       1     0\n",
       "872       0     1\n",
       "873       0     1\n",
       "874       1     0\n",
       "875       1     0\n",
       "876       0     1\n",
       "877       0     1\n",
       "879       1     0\n",
       "880       1     0\n",
       "881       0     1\n",
       "882       1     0\n",
       "883       0     1\n",
       "884       0     1\n",
       "885       1     0\n",
       "886       0     1\n",
       "887       1     0\n",
       "889       0     1\n",
       "890       0     1\n",
       "\n",
       "[714 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to handle Sex such that it's categorical, for logistic regression.\n",
    "# Currently it's a string\n",
    "#refer back to last week's lecture if you forget why we're doing this\n",
    "\n",
    "#We can use pandas get_dummies to implement one hot encoding.\n",
    "pd.get_dummies(X.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT! get_dummies returns an indicator variable for each category.\n",
    "#Refering back to my talk on encoding variables, it's important to drop one category\n",
    "#Otherwise you'll have two perfectly colinear variables.   \n",
    "\n",
    "#Here, since I only have two variables it's easy, I'll just take one, and reassign it to sex\n",
    "#so now Sex becomes female = 1, male = 0\n",
    "X['sex'] = pd.get_dummies(X.sex)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#remember to scale our features, as with linear regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X= scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "At this point I have a test and train set defined.  I will use train to train my model and test to see how accurate the model is.\n",
    "\n",
    "There's one problem with that though.   Lets say my model is right 70% of the time.   Is that good?  Maybe?   \n",
    "\n",
    "I'm going to build a simple 'base rate' model to compare my logistic model to, so we can see if our logistic model is useful or not.  \n",
    "\n",
    "Then, I'll build my logistic model.\n",
    "\n",
    "\n",
    "####Base Rate Model\n",
    "For my baserate model, I'm going to predict that everyone dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function looks for females in the test set and returns 1, survived, otherwise it returns 0\n",
    "def base_rate_model(X):\n",
    "    y = np.zeros(X.shape[0])\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base rate accuracy is 0.61\n"
     ]
    }
   ],
   "source": [
    "#how accurate is my base rate model?\n",
    "y_base_rate = base_rate_model(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Base rate accuracy is %2.2f\" % accuracy_score(y_test, y_base_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our base model is 61% correct, lets see if logistic can beat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy is 0.73\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic accuracy is %2.2f\" % accuracy_score(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "Our base model wasn't very good, but it looked better than it was because of class imbalance.  There are many more 0s than 1s in our dataset, so if we just guess 0 we can 'cheat.'\n",
    "\n",
    "A better metric for binary classifer comparisons is AUC or area under the curve. \n",
    "\n",
    "Closely related is [precision and recall](http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html).\n",
    "\n",
    "Precision is the fraction of correctly identified examples of a class (ratio of true positives to all positives).\n",
    "\n",
    "Recall is the fraction of observastions classified in that class that was correctly classified.  \n",
    "\n",
    "Think of fishing with a net for tuna.   \n",
    "*  If our net is very precise, and has high recall it will catch any and all tuna and ONLY tuna.\n",
    "*  If our net is very precise, but has low recall then we might catch one tuna, but most will escape.\n",
    "*  If our net is low precision, but has high recall, then we might catch tuna, but also any other fish around\n",
    "*  If our net is low precision, and low recall, then we should probably give up fishing.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Base Model---\n",
      "Base Rate AUC = 0.50\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      1.00      0.76        87\n",
      "          1       0.00      0.00      0.00        56\n",
      "\n",
      "avg / total       0.37      0.61      0.46       143\n",
      "\n",
      "\n",
      "\n",
      "---Logistic Model---\n",
      "Logistic AUC = 0.72\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.79      0.78        87\n",
      "          1       0.67      0.64      0.65        56\n",
      "\n",
      "avg / total       0.73      0.73      0.73       143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"---Base Model---\")\n",
    "#base rate AUC\n",
    "base_roc_auc = roc_auc_score(y_test, base_rate_model(X_test))\n",
    "print(\"Base Rate AUC = %2.2f\" % base_roc_auc)\n",
    "print(classification_report(y_test,base_rate_model(X_test) ))\n",
    "print(\"\\n\\n---Logistic Model---\")\n",
    "#logistic AUC\n",
    "logit_roc_auc = roc_auc_score(y_test, model.predict(X_test))\n",
    "print(\"Logistic AUC = %2.2f\" % logit_roc_auc)\n",
    "print(classification_report(y_test, model.predict(X_test) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVPQXSJRJNNK/j1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f45d85c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "X['survived'] = df['Survived']\n",
    "X['pclass'] = df['Pclass']\n",
    "X['sex'] = df['Sex']\n",
    "X['age'] = df['Age']\n",
    "X['fare'] = df['Fare']\n",
    "\n",
    "X = X.dropna(axis=0)\n",
    "\n",
    "y = X['survived']\n",
    "X = X.drop(['survived'], axis=1)\n",
    "X['sex'] = pd.get_dummies(X.sex)['female']\n",
    "#X\n",
    "#X.columns\n",
    "\n",
    "X['pclass'] = X['pclass'].astype('category')\n",
    "#X.dtypes\n",
    "#X['pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---LogisticRegression(C=0.1) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.100150753769) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.100301507538) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.100452261307) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.100603015075) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.100753768844) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.100904522613) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.101055276382) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.101206030151) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.10135678392) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.101507537688) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.101658291457) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.101809045226) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.101959798995) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.102110552764) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.102261306533) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.102412060302) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.10256281407) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.102713567839) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.102864321608) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.103015075377) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.103165829146) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.103316582915) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.103467336683) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.103618090452) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.103768844221) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.10391959799) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.104070351759) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.104221105528) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.104371859296) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.104522613065) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.104673366834) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.104824120603) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.104974874372) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.105125628141) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.10527638191) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.105427135678) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.105577889447) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.105728643216) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.105879396985) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.106030150754) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.106180904523) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.106331658291) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.10648241206) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.106633165829) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.106783919598) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.106934673367) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.107085427136) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.107236180905) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.107386934673) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.107537688442) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.107688442211) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.10783919598) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.107989949749) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.108140703518) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.108291457286) Model---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.108442211055) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.108592964824) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.108743718593) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.108894472362) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.109045226131) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.109195979899) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.109346733668) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.109497487437) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.109648241206) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.109798994975) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.109949748744) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.110100502513) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.110251256281) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.11040201005) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.110552763819) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.110703517588) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.110854271357) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.111005025126) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.111155778894) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.111306532663) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.111457286432) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.111608040201) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.11175879397) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.111909547739) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.112060301508) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.112211055276) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.112361809045) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.112512562814) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.112663316583) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.112814070352) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.112964824121) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.113115577889) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.113266331658) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.113417085427) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.113567839196) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.113718592965) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.113869346734) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.114020100503) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.114170854271) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.11432160804) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.114472361809) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.114623115578) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.114773869347) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.114924623116) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.115075376884) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.115226130653) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.115376884422) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.115527638191) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.11567839196) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.115829145729) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.115979899497) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.116130653266) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.116281407035) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.116432160804) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.116582914573) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.116733668342) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.116884422111) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.117035175879) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.117185929648) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.117336683417) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.117487437186) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.117638190955) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.117788944724) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.117939698492) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.118090452261) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.11824120603) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.118391959799) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.118542713568) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.118693467337) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.118844221106) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.118994974874) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.119145728643) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.119296482412) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.119447236181) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.11959798995) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.119748743719) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.119899497487) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.120050251256) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.120201005025) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.120351758794) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.120502512563) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.120653266332) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.120804020101) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.120954773869) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.121105527638) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.121256281407) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.121407035176) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.121557788945) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.121708542714) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.121859296482) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.122010050251) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.12216080402) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.122311557789) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.122462311558) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.122613065327) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.122763819095) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.122914572864) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.123065326633) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.123216080402) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.123366834171) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.12351758794) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.123668341709) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.123819095477) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.123969849246) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.124120603015) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.124271356784) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.124422110553) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.124572864322) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.12472361809) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.124874371859) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.125025125628) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.125175879397) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.125326633166) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.125477386935) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.125628140704) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.125778894472) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.125929648241) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.12608040201) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.126231155779) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.126381909548) Model---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.126532663317) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.126683417085) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.126834170854) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.126984924623) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.127135678392) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.127286432161) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.12743718593) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.127587939698) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.127738693467) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.127889447236) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.128040201005) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.128190954774) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.128341708543) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.128492462312) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.12864321608) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.128793969849) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.128944723618) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.129095477387) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.129246231156) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.129396984925) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.129547738693) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.129698492462) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.129849246231) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n",
      "---LogisticRegression(C=0.13) Model---\n",
      "LogisticRegression Rate AUC = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.79      0.80        87\n",
      "          1       0.68      0.70      0.69        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_report(model, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    name = type(model).__name__\n",
    "    print(\"---%s(C=%s) Model---\" % (name, model.C))\n",
    "    #rate AUC\n",
    "    roc_auc = roc_auc_score(y_test, model.predict(X_test))\n",
    "    print(\"%s Rate AUC = %2.2f\" % (name, roc_auc))\n",
    "    print(classification_report(y_test,model.predict(X_test)))\n",
    "    \n",
    "import numpy\n",
    "\n",
    "for c in numpy.linspace(0.1, 0.13, 200):\n",
    "    model_report(LogisticRegression(penalty='l2', C=c), X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---LogisticRegression(C=0.1) Model---\n",
      "LogisticRegression Rate AUC = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        87\n",
      "          1       0.70      0.70      0.70        56\n",
      "\n",
      "avg / total       0.76      0.76      0.76       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(LogisticRegression(penalty='l2', C=0.1), X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
